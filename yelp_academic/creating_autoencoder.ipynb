{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import twokenize\n",
    "import unidecode\n",
    "\n",
    "# We now use the data from Santa Barbara and go through the NLP pipeline steps\n",
    "reviews = pd.read_csv('csv_data/santa_barbara_reviews.csv')"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0  this easter instead of going to lopez lake we ...   \n1  had a party of 6 here for hibachi. our waitres...   \n2  what a great addition to the funk zone!  grab ...   \n3  farmhouse, rustic, chic.helpful staff with gre...   \n4  we were a bit weary about trying the shellfish...   \n\n                                         spacy_token  \\\n0  [this, easter, instead, of, going, to, lopez, ...   \n1  [had, a, party, of, 6, here, for, hibachi, ., ...   \n2  [what, a, great, addition, to, the, funk, zone...   \n3  [farmhouse, ,, rustic, ,, chic.helpful, staff,...   \n4  [we, were, a, bit, weary, about, trying, the, ...   \n\n                                     twokenize_token  \n0  [this, easter, instead, of, going, to, lopez, ...  \n1  [had, a, party, of, 6, here, for, hibachi, ., ...  \n2  [what, a, great, addition, to, the, funk, zone...  \n3  [farmhouse, ,, rustic, ,, chic, ., helpful, st...  \n4  [we, were, a, bit, weary, about, trying, the, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>spacy_token</th>\n      <th>twokenize_token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>this easter instead of going to lopez lake we ...</td>\n      <td>[this, easter, instead, of, going, to, lopez, ...</td>\n      <td>[this, easter, instead, of, going, to, lopez, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>had a party of 6 here for hibachi. our waitres...</td>\n      <td>[had, a, party, of, 6, here, for, hibachi, ., ...</td>\n      <td>[had, a, party, of, 6, here, for, hibachi, ., ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>what a great addition to the funk zone!  grab ...</td>\n      <td>[what, a, great, addition, to, the, funk, zone...</td>\n      <td>[what, a, great, addition, to, the, funk, zone...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>farmhouse, rustic, chic.helpful staff with gre...</td>\n      <td>[farmhouse, ,, rustic, ,, chic.helpful, staff,...</td>\n      <td>[farmhouse, ,, rustic, ,, chic, ., helpful, st...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>we were a bit weary about trying the shellfish...</td>\n      <td>[we, were, a, bit, weary, about, trying, the, ...</td>\n      <td>[we, were, a, bit, weary, about, trying, the, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing accents and making the text lowercase\n",
    "reviews['text'] = [unidecode.unidecode(review_text).lower() for review_text in reviews['text']]\n",
    "\n",
    "# tokenizing with spacy\n",
    "spacy_tokenizer = English()\n",
    "reviews['spacy_token'] = [[token.text for token in spacy_tokenizer(review_text)] for review_text in reviews['text']]\n",
    "\n",
    "# tokenizing with twokenize\n",
    "reviews['twokenize_token'] = [twokenize.tokenizeRawTweetText(review_text) for review_text in reviews['text']]\n",
    "\n",
    "reviews[['text', 'spacy_token', 'twokenize_token']].head()  # check results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0  this easter instead of going to lopez lake we ...   \n1  had a party of 6 here for hibachi. our waitres...   \n2  what a great addition to the funk zone!  grab ...   \n3  farmhouse, rustic, chic.helpful staff with gre...   \n4  we were a bit weary about trying the shellfish...   \n\n                                         spacy_token  \\\n0  [easter, instead, going, lopez, lake, went, lo...   \n1  [party, 6, hibachi, ., waitress, brought, sepa...   \n2  [great, addition, funk, zone, !,  , grab, bite...   \n3  [farmhouse, ,, rustic, ,, chic.helpful, staff,...   \n4  [bit, weary, trying, shellfish, company, wharf...   \n\n                                     twokenize_token  \n0  [easter, instead, going, lopez, lake, went, lo...  \n1  [party, 6, hibachi, ., waitress, brought, sepa...  \n2  [great, addition, funk, zone, !, grab, bite, ,...  \n3  [farmhouse, ,, rustic, ,, chic, ., helpful, st...  \n4  [bit, weary, trying, shellfish, company, wharf...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>spacy_token</th>\n      <th>twokenize_token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>this easter instead of going to lopez lake we ...</td>\n      <td>[easter, instead, going, lopez, lake, went, lo...</td>\n      <td>[easter, instead, going, lopez, lake, went, lo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>had a party of 6 here for hibachi. our waitres...</td>\n      <td>[party, 6, hibachi, ., waitress, brought, sepa...</td>\n      <td>[party, 6, hibachi, ., waitress, brought, sepa...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>what a great addition to the funk zone!  grab ...</td>\n      <td>[great, addition, funk, zone, !,  , grab, bite...</td>\n      <td>[great, addition, funk, zone, !, grab, bite, ,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>farmhouse, rustic, chic.helpful staff with gre...</td>\n      <td>[farmhouse, ,, rustic, ,, chic.helpful, staff,...</td>\n      <td>[farmhouse, ,, rustic, ,, chic, ., helpful, st...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>we were a bit weary about trying the shellfish...</td>\n      <td>[bit, weary, trying, shellfish, company, wharf...</td>\n      <td>[bit, weary, trying, shellfish, company, wharf...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stop words\n",
    "reviews['spacy_token'] = [list(filter(lambda word: word not in STOP_WORDS, list_of_tokens)) for list_of_tokens in reviews['spacy_token']]\n",
    "reviews['twokenize_token'] = [list(filter(lambda word: word not in STOP_WORDS, list_of_tokens)) for list_of_tokens in reviews['twokenize_token']]\n",
    "reviews[['text', 'spacy_token', 'twokenize_token']].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# the first thing we try is a vectorization using bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer(max_features=1024, ngram_range=(1,2))\n",
    "bow_input = [' '.join(review) for review in reviews['spacy_token'].tolist()]\n",
    "bow_representation = count_vec.fit_transform(bow_input)   # fitting the model\n",
    "bow_array = bow_representation.toarray()\n",
    "normalized_bow = [vector/sum(vector) if sum(vector) != 0 else vector for vector in bow_array]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 16)                4112      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               4352      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 1024)              263168    \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,633,232\n",
      "Trainable params: 2,633,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.losses import CosineSimilarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=1024, activation=\"relu\"))\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.summary()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(normalized_bow), np.array(normalized_bow), test_size = 0.25)\n",
    "model.compile(loss=CosineSimilarity(axis=1), optimizer='sgd', metrics=['mse'])\n",
    "plot_model(model, to_file='autoencoder.png', show_shapes=True, show_layer_names=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6320/6320 [==============================] - 100s 16ms/step - loss: -0.2843 - mse: 7.6240e-05 - val_loss: -0.3594 - val_mse: 5.5597e-05\n",
      "Epoch 2/50\n",
      "6320/6320 [==============================] - 82s 13ms/step - loss: -0.4028 - mse: 5.3126e-05 - val_loss: -0.4241 - val_mse: 5.1995e-05\n",
      "Epoch 3/50\n",
      "6320/6320 [==============================] - 82s 13ms/step - loss: -0.4447 - mse: 5.0731e-05 - val_loss: -0.4435 - val_mse: 5.0727e-05\n",
      "Epoch 4/50\n",
      "6320/6320 [==============================] - 81s 13ms/step - loss: -0.4646 - mse: 4.9611e-05 - val_loss: -0.4700 - val_mse: 4.9431e-05\n",
      "Epoch 5/50\n",
      "6320/6320 [==============================] - 118s 19ms/step - loss: -0.4775 - mse: 4.8889e-05 - val_loss: -0.4759 - val_mse: 4.9070e-05\n",
      "Epoch 6/50\n",
      "6320/6320 [==============================] - 122s 19ms/step - loss: -0.4865 - mse: 4.8326e-05 - val_loss: -0.4798 - val_mse: 4.8720e-05\n",
      "Epoch 7/50\n",
      "6320/6320 [==============================] - 99s 16ms/step - loss: -0.4925 - mse: 4.7872e-05 - val_loss: -0.4864 - val_mse: 4.8193e-05\n",
      "Epoch 8/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.4967 - mse: 4.7552e-05 - val_loss: -0.4903 - val_mse: 4.7921e-05\n",
      "Epoch 9/50\n",
      "6320/6320 [==============================] - 82s 13ms/step - loss: -0.4999 - mse: 4.7307e-05 - val_loss: -0.4912 - val_mse: 4.7846e-05\n",
      "Epoch 10/50\n",
      "6320/6320 [==============================] - 86s 14ms/step - loss: -0.5025 - mse: 4.7127e-05 - val_loss: -0.5004 - val_mse: 4.7320e-05\n",
      "Epoch 11/50\n",
      "6320/6320 [==============================] - 96s 15ms/step - loss: -0.5045 - mse: 4.6999e-05 - val_loss: -0.5011 - val_mse: 4.7362e-05\n",
      "Epoch 12/50\n",
      "6320/6320 [==============================] - 92s 14ms/step - loss: -0.5064 - mse: 4.6890e-05 - val_loss: -0.5027 - val_mse: 4.7208e-05\n",
      "Epoch 13/50\n",
      "6320/6320 [==============================] - 85s 14ms/step - loss: -0.5080 - mse: 4.6824e-05 - val_loss: -0.5045 - val_mse: 4.7134e-05\n",
      "Epoch 14/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5095 - mse: 4.6769e-05 - val_loss: -0.5061 - val_mse: 4.7163e-05\n",
      "Epoch 15/50\n",
      "6320/6320 [==============================] - 93s 15ms/step - loss: -0.5108 - mse: 4.6737e-05 - val_loss: -0.5071 - val_mse: 4.7111e-05\n",
      "Epoch 16/50\n",
      "6320/6320 [==============================] - 94s 15ms/step - loss: -0.5118 - mse: 4.6725e-05 - val_loss: -0.5090 - val_mse: 4.6878e-05\n",
      "Epoch 17/50\n",
      "6320/6320 [==============================] - 110s 17ms/step - loss: -0.5128 - mse: 4.6724e-05 - val_loss: -0.5121 - val_mse: 4.6968e-05\n",
      "Epoch 18/50\n",
      "6320/6320 [==============================] - 92s 15ms/step - loss: -0.5137 - mse: 4.6741e-05 - val_loss: -0.5098 - val_mse: 4.7293e-05\n",
      "Epoch 19/50\n",
      "6320/6320 [==============================] - 106s 17ms/step - loss: -0.5146 - mse: 4.6763e-05 - val_loss: -0.5137 - val_mse: 4.7044e-05\n",
      "Epoch 20/50\n",
      "6320/6320 [==============================] - 92s 15ms/step - loss: -0.5154 - mse: 4.6795e-05 - val_loss: -0.5111 - val_mse: 4.7507e-05\n",
      "Epoch 21/50\n",
      "6320/6320 [==============================] - 106s 17ms/step - loss: -0.5161 - mse: 4.6840e-05 - val_loss: -0.5144 - val_mse: 4.7189e-05\n",
      "Epoch 22/50\n",
      "6320/6320 [==============================] - 89s 14ms/step - loss: -0.5168 - mse: 4.6901e-05 - val_loss: -0.5141 - val_mse: 4.7341e-05\n",
      "Epoch 23/50\n",
      "6320/6320 [==============================] - 83s 13ms/step - loss: -0.5174 - mse: 4.6951e-05 - val_loss: -0.5135 - val_mse: 4.7958e-05\n",
      "Epoch 24/50\n",
      "6320/6320 [==============================] - 79s 13ms/step - loss: -0.5180 - mse: 4.7021e-05 - val_loss: -0.5140 - val_mse: 4.7857e-05\n",
      "Epoch 25/50\n",
      "6320/6320 [==============================] - 102s 16ms/step - loss: -0.5186 - mse: 4.7099e-05 - val_loss: -0.5168 - val_mse: 4.7342e-05\n",
      "Epoch 26/50\n",
      "6320/6320 [==============================] - 165s 26ms/step - loss: -0.5191 - mse: 4.7170e-05 - val_loss: -0.5124 - val_mse: 4.7471e-05\n",
      "Epoch 27/50\n",
      "6320/6320 [==============================] - 165s 26ms/step - loss: -0.5196 - mse: 4.7274e-05 - val_loss: -0.5158 - val_mse: 4.7879e-05\n",
      "Epoch 28/50\n",
      "6320/6320 [==============================] - 161s 25ms/step - loss: -0.5200 - mse: 4.7359e-05 - val_loss: -0.5192 - val_mse: 4.7940e-05\n",
      "Epoch 29/50\n",
      "6320/6320 [==============================] - 156s 25ms/step - loss: -0.5206 - mse: 4.7446e-05 - val_loss: -0.5194 - val_mse: 4.7839e-05\n",
      "Epoch 30/50\n",
      "6320/6320 [==============================] - 144s 23ms/step - loss: -0.5210 - mse: 4.7539e-05 - val_loss: -0.5166 - val_mse: 4.8082e-05\n",
      "Epoch 31/50\n",
      "6320/6320 [==============================] - 90s 14ms/step - loss: -0.5215 - mse: 4.7623e-05 - val_loss: -0.5168 - val_mse: 4.8403e-05\n",
      "Epoch 32/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5220 - mse: 4.7705e-05 - val_loss: -0.5110 - val_mse: 4.8203e-05\n",
      "Epoch 33/50\n",
      "6320/6320 [==============================] - 89s 14ms/step - loss: -0.5222 - mse: 4.7813e-05 - val_loss: -0.5189 - val_mse: 4.8088e-05\n",
      "Epoch 34/50\n",
      "6320/6320 [==============================] - 89s 14ms/step - loss: -0.5220 - mse: 4.7875e-05 - val_loss: -0.5171 - val_mse: 4.7950e-05\n",
      "Epoch 35/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5225 - mse: 4.7843e-05 - val_loss: -0.5204 - val_mse: 4.8012e-05\n",
      "Epoch 36/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5226 - mse: 4.7928e-05 - val_loss: -0.5201 - val_mse: 4.8651e-05\n",
      "Epoch 37/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5227 - mse: 4.8011e-05 - val_loss: -0.5203 - val_mse: 4.8216e-05\n",
      "Epoch 38/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5197 - mse: 4.9807e-05 - val_loss: -0.5178 - val_mse: 5.0773e-05\n",
      "Epoch 39/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5215 - mse: 4.8918e-05 - val_loss: -0.5184 - val_mse: 4.8542e-05\n",
      "Epoch 40/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5227 - mse: 4.8306e-05 - val_loss: -0.5173 - val_mse: 4.8607e-05\n",
      "Epoch 41/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5226 - mse: 4.8325e-05 - val_loss: -0.5199 - val_mse: 4.8524e-05\n",
      "Epoch 42/50\n",
      "6320/6320 [==============================] - 89s 14ms/step - loss: -0.5231 - mse: 4.8089e-05 - val_loss: -0.5193 - val_mse: 4.8639e-05\n",
      "Epoch 43/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5225 - mse: 4.7999e-05 - val_loss: -0.5182 - val_mse: 4.8105e-05\n",
      "Epoch 44/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5231 - mse: 4.7969e-05 - val_loss: -0.5206 - val_mse: 4.8818e-05\n",
      "Epoch 45/50\n",
      "6320/6320 [==============================] - 89s 14ms/step - loss: -0.5233 - mse: 4.7931e-05 - val_loss: -0.5213 - val_mse: 4.8242e-05\n",
      "Epoch 46/50\n",
      "6320/6320 [==============================] - 89s 14ms/step - loss: -0.5236 - mse: 4.7724e-05 - val_loss: -0.5219 - val_mse: 4.8228e-05\n",
      "Epoch 47/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5232 - mse: 4.7730e-05 - val_loss: -0.5210 - val_mse: 4.7928e-05\n",
      "Epoch 48/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5240 - mse: 4.7509e-05 - val_loss: -0.5216 - val_mse: 4.7918e-05\n",
      "Epoch 49/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5241 - mse: 4.7358e-05 - val_loss: -0.5208 - val_mse: 4.7246e-05\n",
      "Epoch 50/50\n",
      "6320/6320 [==============================] - 88s 14ms/step - loss: -0.5244 - mse: 4.7223e-05 - val_loss: -0.5070 - val_mse: 5.0913e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fe6e48db3d0>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=35, validation_data=(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: autoencoder_v1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('autoencoder_v1')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# we then move to the TF-IDF vectorization\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf = TfidfVectorizer()\n",
    "# tfidf_representation = tfidf.fit_transform(bow_input)   # fitting the model\n",
    "# tfidf_array = tfidf_representation.toarray()\n",
    "# normalized_tfidf = [vector/sum(vector) if sum(vector) != 0 else vector for vector in tfidf_array]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# normalized_tfidf - normalized_bow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}